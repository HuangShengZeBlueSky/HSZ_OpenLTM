import os
import torch
import numpy as np
import torch.nn as nn
import time
from exp.exp_forecast import Exp_Forecast

class Exp_Forecast_TestAll3(Exp_Forecast):
    def __init__(self, args):
        super(Exp_Forecast_TestAll3, self).__init__(args)

    def test(self, setting, test=0):
            # =========================================
            # 1. åŠ è½½æ¨¡å‹
            # =========================================
            if test:
                print('loading model')
                setting = self.args.test_dir
                best_model_path = self.args.test_file_name
                checkpoint_path = os.path.join(self.args.checkpoints, setting, best_model_path)
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                self.model.load_state_dict(checkpoint, strict=False)
            self.model.eval()
            criterion = nn.MSELoss(reduction='none') 

            # =========================================
            # 2. å‡†å¤‡æ•°æ®è¯»å–å™¨ (åªä¸ºäº†è·å– scaler)
            # =========================================
            # æˆ‘ä»¬ä»éœ€è¦è°ƒç”¨ä¸€æ¬¡ _get_data æ¥è·å–è®­ç»ƒæ—¶çš„ scalerï¼Œä½†æˆ‘ä»¬ä¸å†ä¿¡ä»»å®ƒçš„æ•°æ®è´¨é‡
            _, temp_loader = self._get_data(flag='val')
            train_scaler = temp_loader.dataset.scaler

            # =========================================
            # 3. éå†æµ‹è¯•é›† & åŠ¨æ€æ„å»ºâ€œåŸºå‡†æ± â€
            # =========================================
            print(">>> é˜¶æ®µ1: æ­£åœ¨è¿›è¡Œæ¨¡å‹æ¨ç†...")
            
            current_root = self.args.root_path
            if current_root.endswith('/test') or current_root.endswith('/test/'):
                test_folder = current_root 
            else:
                test_folder = os.path.join(current_root, 'test')
                
            self.args.root_path = test_folder 
            csv_files = sorted([f for f in os.listdir(test_folder) if f.endswith(".csv")])
            
            test_data_map = {}      # å­˜æ‰€æœ‰æ–‡ä»¶çš„ Loss
            calibration_losses = [] # [å…³é”®ä¿®æ”¹] ä¸“é—¨å­˜â€œæ­£å¸¸æ–‡ä»¶â€çš„ Lossï¼Œä»£æ›¿éªŒè¯é›†
            
            # ä½ çš„æ­£å¸¸æ–‡ä»¶å…³é”®è¯
            normal_keywords = ["æ­£å¸¸", "normal", "m0_g0_la0_ra0"]
            
            print(f"    å¤„ç†æµ‹è¯•é›† ({len(csv_files)} ä¸ªæ–‡ä»¶)...")
            
            with torch.no_grad():
                for csv_file in csv_files:
                    _, test_loader = self._get_data(flag='test', test_data_path=csv_file, scaler=train_scaler)
                    file_losses = []
                    
                    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
                        batch_x = batch_x.float().to(self.device)
                        batch_y = batch_y.float().to(self.device)
                        batch_x_mark = batch_x_mark.float().to(self.device)
                        batch_y_mark = batch_y_mark.float().to(self.device)
                        
                        outputs = self.model(batch_x, batch_x_mark, batch_y_mark)
                        f_dim = -1 if getattr(self.args, 'features', 'M') == 'MS' else 0
                        outputs = outputs[:, -self.args.output_token_len:, f_dim:]
                        batch_y = batch_y[:, -self.args.output_token_len:, f_dim:]
                        
                        loss = criterion(outputs, batch_y).mean(dim=-1)
                        file_losses.append(loss.cpu().numpy())
                    
                    if len(file_losses) > 0:
                        # 1. å­˜å…¥æ€»è¡¨
                        file_loss_concat = np.concatenate(file_losses, axis=0)
                        test_data_map[csv_file] = file_loss_concat
                        
                        # 2. [æ ¸å¿ƒä¿®å¤] å¦‚æœæ˜¯æ­£å¸¸æ–‡ä»¶ï¼ŒåŠ å…¥åˆ°â€œåŸºå‡†æ± â€
                        # åªæœ‰è¿™é‡Œçš„ loss æ‰æ˜¯å¹²å‡€çš„ (0.02çº§åˆ«)ï¼Œæˆ‘ä»¬è¦ç”¨å®ƒæ¥å®šé˜ˆå€¼
                        if any(k.lower() in csv_file.lower() for k in normal_keywords):
                            calibration_losses.append(file_loss_concat)

            # =========================================
            # 4. æ„å»ºæ–°çš„åŸºå‡† (Calibration Data)
            # =========================================
            if len(calibration_losses) > 0:
                # ä½¿ç”¨æ‰¾åˆ°çš„æ­£å¸¸æ–‡ä»¶ä½œä¸ºåŸºå‡†
                val_loss_matrix = np.concatenate(calibration_losses, axis=0)
                val_flattened = val_loss_matrix.flatten()
                print(f"    âœ… å·²è‡ªåŠ¨è¯†åˆ« {len(calibration_losses)} ä¸ªæ­£å¸¸æ–‡ä»¶ä½œä¸ºé˜ˆå€¼åŸºå‡†ã€‚")
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æ­£å¸¸æ–‡ä»¶ï¼Œè¿™å°±æ˜¯ç¾éš¾ï¼Œåªèƒ½æŠ¥é”™äº†
                print("    âŒ ä¸¥é‡é”™è¯¯ï¼šæµ‹è¯•é›†ä¸­æœªæ‰¾åˆ°åŒ…å« 'æ­£å¸¸/normal' çš„æ–‡ä»¶ï¼æ— æ³•è®¡ç®—é˜ˆå€¼ã€‚")
                return

            # =========================================
            # 5. è¯Šæ–­ï¼šMSE åŸå§‹åˆ†å¸ƒæ£€æŸ¥
            # =========================================
            print("\n>>> é˜¶æ®µ1.5: åŸå§‹è¯¯å·®(MSE)è¯Šæ–­")
            
            normal_mses = []
            fault_mses = []
            
            print("-" * 70)
            print(f"{'File Name':<55} | {'Avg MSE'}")
            print("-" * 70)
            
            # æ‰“å°åŸºå‡†æ± çš„ MSE (ç°åœ¨åº”è¯¥æ˜¯ 0.02 å·¦å³äº†)
            baseline_mse = np.mean(val_flattened)
            print(f"{'[New Baseline] (From Normal Files)':<55} | {baseline_mse:.6f}")
            
            for fname, loss_m in test_data_map.items():
                avg_mse = loss_m.mean()
                is_normal = any(k.lower() in fname.lower() for k in normal_keywords)
                tag = "[æ­£å¸¸]" if is_normal else "[æ•…éšœ]"
                
                print(f"{tag + ' ' + fname[:45]:<55} | {avg_mse:.6f}")
                
                if is_normal: normal_mses.append(avg_mse)
                else: fault_mses.append(avg_mse)
                
            print("-" * 70)
            
            # ç®€å•çš„è‡ªåŠ¨åˆ¤æ–­
            max_norm_mse = max(normal_mses) if normal_mses else 0
            min_fault_mse = min(fault_mses) if fault_mses else 0
            
            if min_fault_mse <= max_norm_mse:
                print(f"âš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†æ•…éšœæ ·æœ¬MSE ({min_fault_mse:.4f}) <= æ­£å¸¸æ ·æœ¬MSE ({max_norm_mse:.4f})")
                print(f"    éƒ¨åˆ†è½»å¾®æ•…éšœå¯èƒ½éš¾ä»¥æ£€æµ‹ã€‚")
            else:
                print(f"âœ… ä¿¡å·è‰¯å¥½ï¼šæ•…éšœMSE > æ­£å¸¸MSEã€‚")

            # =========================================
            # 6. å¯»ä¼˜é˜¶æ®µ (Top-8)
            # =========================================
            print("\n>>> é˜¶æ®µ2: å†…å­˜å‚æ•°å¯»ä¼˜ (Top-8 å±•ç¤º)...")
            print(f"    [è§„åˆ™] æ’åºä¾æ®: Gap (å¹³å‡æ£€å‡º - å¹³å‡è¯¯æŠ¥) é™åº")
            print(f"    [ç¡¬çº¦æŸ] æ­£å¸¸æ ·æœ¬å¹³å‡è¯¯æŠ¥ç‡ <= 10%")
            
            p_list = [99.9, 99.5, 99.0, 98.0, 96.0, 95.0, 94.0, 92.0, 90.0, 85.0, 80.0]
            v_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.10, 0.12, 0.15]
            
            candidates = [] 
            
            for p in p_list:
                # [å…³é”®] ç°åœ¨ thresh æ˜¯åŸºäºçœŸæ­£çš„æ­£å¸¸æ–‡ä»¶è®¡ç®—çš„ (0.02çº§åˆ«)
                # è¿™æ ·å³ä½¿æ˜¯ 0.024 çš„æ•…éšœï¼Œä¹Ÿæœ‰æœºä¼šè¢«æŠ“å‡ºæ¥
                thresh = np.percentile(val_flattened, p)
                
                for v in v_list:
                    test_normal_rates = [] 
                    fault_rates = []       
                    current_results = {}
                    
                    # a. åŸºå‡†æ±  FPR (è‡ªæˆ‘éªŒè¯)
                    val_anomalies = (val_loss_matrix > thresh)
                    val_fpr = (val_anomalies.sum(axis=1) > (val_loss_matrix.shape[1] * v)).mean()
                    if val_fpr > 0.10: continue 
                    
                    current_results["[Baseline]"] = val_fpr

                    # b. æµ‹è¯•é›†éå†
                    for fname, loss_m in test_data_map.items():
                        is_anomaly = (loss_m > thresh).sum(axis=1) > (loss_m.shape[1] * v)
                        rate = is_anomaly.mean()
                        current_results[fname] = rate
                        
                        if any(k.lower() in fname.lower() for k in normal_keywords):
                            test_normal_rates.append(rate)
                        else:
                            fault_rates.append(rate)
                    
                    if not fault_rates: continue

                    avg_norm_rate = np.mean(test_normal_rates)
                    avg_fault_rate = np.mean(fault_rates)
                    
                    if avg_norm_rate > 0.10: continue
                    
                    gap = avg_fault_rate - avg_norm_rate
                    
                    candidates.append({
                        'p': p, 'v': v,
                        'gap': gap,
                        'avg_norm': avg_norm_rate,
                        'avg_fault': avg_fault_rate,
                        'results': current_results
                    })

            # =========================================
            # 7. æ’åºä¸è¾“å‡º
            # =========================================
            candidates.sort(key=lambda x: -x['gap'])
            
            top_k = min(8, len(candidates))
            
            if top_k == 0:
                print("âŒ å¯»ä¼˜å¤±è´¥ï¼æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ»¡è¶³ã€è¯¯æŠ¥ç‡<=10%ã€‘çš„å‚æ•°ç»„åˆã€‚")
            else:
                print(f"\nğŸ“Š ç­›é€‰å‡º {len(candidates)} ç»„æœ‰æ•ˆå‚æ•°ï¼Œå±•ç¤ºå‰ {top_k} ç»„è¯¦ç»†ç»“æœ")
                
                for rank in range(top_k):
                    cand = candidates[rank]
                    p, v = cand['p'], cand['v']
                    
                    print("\n" + "=" * 80)
                    print(f"ğŸ… Rank {rank+1}:  P={p} | V={v} | Gap={cand['gap']:.4f}")
                    print(f"   (Avg Norm: {cand['avg_norm']*100:.2f}% | Avg Fault: {cand['avg_fault']*100:.2f}%)")
                    print("-" * 80)
                    print(f"{'File Name':<60} | {'Anomaly Rate'}")
                    print("-" * 80)
                    
                    sorted_items = sorted(cand['results'].items(), key=lambda x: (
                        0 if "[Base" in x[0] else (1 if any(k in x[0].lower() for k in normal_keywords) else 2), 
                        x[0]
                    ))
                    
                    for fname, rate in sorted_items:
                        print(f"{fname:<60} | {rate*100:6.2f}%")
                    
                print("=" * 80)
                
                best_cand = candidates[0]
                out_root = './test_results/' + setting + '/'
                if not os.path.exists(out_root): os.makedirs(out_root)
                np.save(os.path.join(out_root, "best_results.npy"), best_cand['results'])
                print(f"\nâœ… å·²è‡ªåŠ¨é€‰æ‹© Rank 1 (P={best_cand['p']}, V={best_cand['v']}) ä½œä¸ºæœ€ä½³ç»“æœä¿å­˜ã€‚")

            self.args.root_path = os.path.dirname(test_folder.rstrip('/'))