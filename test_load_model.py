import torch
from transformers import LlamaForCausalLM

# è¿™æ˜¯ä½ åˆšåˆšä¸‹è½½å¥½çš„ç»å¯¹è·¯å¾„
MODEL_PATH = "/home/ubuntu/hsz/models/NousResearch/Llama-2-7b-hf"

print(f"ğŸš€ å¼€å§‹æµ‹è¯•åŠ è½½ Llama-2ï¼Œè·¯å¾„: {MODEL_PATH}")
print("... æ­£åœ¨åŠ è½½åˆ°å†…å­˜ (é¢„è®¡æ¶ˆè€—çº¦ 13GB å†…å­˜) ...")

try:
    # å…³é”®ç‚¹ï¼šè¿™é‡Œæˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨ float16ï¼Œæ¨¡æ‹Ÿåœ¨ 4090 ä¸Šçš„æ˜¾å­˜å ç”¨æƒ…å†µ
    model = LlamaForCausalLM.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.float16, 
        low_cpu_mem_usage=True
    )
    print("âœ… åŠ è½½æˆåŠŸï¼æ¨¡å‹æ–‡ä»¶å®Œæ•´ã€‚")
    
    # æ‰“å°ä¸€ä¸‹æ¨¡å‹å‚æ•°é‡
    params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {params / 1e9:.2f} Billion")
    
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥ï¼é”™è¯¯ä¿¡æ¯:\n{e}")